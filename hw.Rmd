---
title: "Topics_2"
author: "Domagoj Fizulic"
date: "May 20, 2016"
output: pdf_document
---

# Question 1

Power 1 is the black line. Other colors are powers 2-5.

```{r, echo=FALSE, message=FALSE}
setwd("~/bgse/15D013_Topics_2/Prof. A. Arratia")

library(quantmod); library(xts)
library(fBasics)

acplot <- function(sym){
  
  dat <- getSymbols(sym, auto.assign = FALSE)
  df <- as.data.frame(dat)
  df$rtrn <- 0
  
  column <- paste(c(sym,".Adjusted"),collapse="")
  
  for(i in 2:nrow(df)){
    df[i,"rtrn"] <- abs(df[i,column]/df[(i-1),column]-1)
  }
  
  #teffectPlot(df$rtrn)
  
  df$rtrn2 <- df$rtrn^2
  df$rtrn3 <- df$rtrn^3
  df$rtrn4 <- df$rtrn^4
  df$rtrn5 <- df$rtrn^5
  
  
  ac <- acf(df$rtrn,plot = FALSE)
  ac2 <- acf(df$rtrn2,plot = FALSE)
  ac3 <- acf(df$rtrn3,plot = FALSE)
  ac4 <- acf(df$rtrn4,plot = FALSE)
  ac5 <- acf(df$rtrn5,plot = FALSE)
  
  plot(ac$acf,type="l")
  title(main = sym)
  lines(ac2$acf, col="red")
  lines(ac3$acf, col="blue")
  lines(ac4$acf, col="yellow")
  lines(ac5$acf, col="orange")
  
  teffectPlot(df$rtrn)
}

#acplot("AAPL")
#acplot("GOOGL")
#acplot("GSPC")

```

## Apple
```{r, echo=FALSE, message=FALSE}
acplot("AAPL")
```

## Google
```{r, echo=FALSE, message=FALSE}
acplot("GOOGL")
```

## S&P 500
```{r, echo=FALSE, message=FALSE}
acplot("GSPC")
```

# Question 2

```{r, echo=FALSE, message=FALSE}
getSymbols("UMCSENT",src="FRED")

df <- as.data.frame(UMCSENT)
plot(df$UMCSENT,type="l")

ac <- acf(df$UMCSENT)

pac <- pacf(df$UMCSENT)

```

From above plots we ca see the data is not stationary. Confirm with Dickey-Fuller.

```{r, echo=FALSE, message=FALSE}
#unit roots test
library(tseries)
adf.test(df$UMCSENT) #non-stationary, cannot fit ARIMA
```

## Difference the data

```{r, echo=FALSE, message=FALSE}
plot(diff(df$UMCSENT), type="l")
df_diff <- diff(df$UMCSENT)
acf(df_diff)
pacf(df_diff)
adf.test(df_diff)
```


## Auto ARIMA wrt AIC

```{r, echo=FALSE, message=FALSE}
library(forecast)
best_ar <- auto.arima(df_diff)
best_ar
```

## Square of returns of UMCSENT
```{r, echo=FALSE,message=FALSE}
df$rtrn <- 0

for(i in 2:nrow(df)){
  df[i,"rtrn"] <- (df[i,"UMCSENT"]/df[(i-1),"UMCSENT"]-1)
}

df$rtrn2 <- df$rtrn^2
```

## ACF & PACF
```{r, echo=FALSE,message=FALSE}
acf(df$rtrn2)
pacf(df$rtrn2)
```

## Ljung-Box
```{r, echo=FALSE,message=FALSE}
Box.test(df$rtrn2, type = "Ljung-Box")
```

## ARMA-GARCH
```{r, echo=FALSE,message=FALSE, results='hide'}
library(fGarch)
arma_gar <- garchFit(formula = ~arma(1,1)+garch(1,1), data = df$rtrn)
```

```{r, echo=FALSE,message=FALSE}
summary(arma_gar)
```

# Question 3

```{r, echo=FALSE,message=FALSE}

log_return <- function(vec){
  
  #v1
  rtrn <- rep(0,length(vec))
  
  for(i in 2:length(vec)){
    rtrn[i] <- vec[i]/vec[(i-1)]-1
  }
  
  #logrtrn <- 100*log(1+rtrn)
  logrtrn <- log(1+rtrn)
  
  #v2
  #   logrtrn <- rep(0,length(vec))
  #   for(i in 2:length(vec)){
  #     logrtrn[i] <- log(vec[i]) - log(vec[i-1])
  #   }
  #   logrtrn <- logrtrn*100
  
  return(logrtrn)
}


lagdata <- function(set,lag){
  x <- data.frame(set[,1])
  for(i in 1:lag){
    y <- set[-c(nrow(set):(nrow(set)-i+1)),-1]
    x <- data.frame(x[-1,],y)
  }
  return(x)
}


sp <- read.csv("data/data/SP500_shiller.csv")

divpr <- sp$Real.Dividend/sp$Real.Price


splogrtrn <- log_return(sp$SP500)


dta <- data.frame(log_return(sp$SP500),log_return(divpr))
dta1 <- data.frame(log_return(sp$SP500),log_return(sp$P.E10))
dta2 <- data.frame(log_return(sp$SP500),log_return(divpr),log_return(sp$P.E10))

dta1 <- dta1[complete.cases(dta1),]
dta2 <- dta2[complete.cases(dta2),]


training <- lagdata(dta,5)
#head(training)
names <- names(training)
names[1] <- "target"
names(training) <- names

training1 <- lagdata(dta1,5)
#head(training1)
names <- names(training1)
names[1] <- "target"
names(training1) <- names

training2 <- lagdata(dta2,5)
#head(training2)
names <- names(training2)
names[1] <- "target"
names(training2) <- names

cv.nn <- function(train, size=10, rounds=250){

  library(nnet)
  library(Metrics)

  c <- which(colnames(train)=="target")

    library(caret)
    set.seed(1234)
    flds <- createFolds(train$target,k=5)
    nnpred <- rep(NA,5)

    for(j in 1:5){
      ktest <- train[flds[[j]],]
      ktrain <- train[-flds[[j]],]

      #print("fold")
      #print(j) # tracing progress
      #print(nnpred)

      size=6
      nnetFit = nnet(ktrain[,-1], ktrain[,1],
               size=size,skip=T, maxit=10^4,decay=10^{-2},trace=F,linout=T)

      #nnetFit = nnet(ktrain[,-1], ktrain[,1],
      #         size=size, linout=T)

      prednet<-predict(nnetFit,ktest[,-1],type="raw")


      #print(mse(ktest[,1],prednet))

      nnpred[j] <- mse(ktest[,1],prednet)

    } # end of looping cross validations
    avg <- mean(nnpred)

    #print(nnpred)
    #print(avg)
    return(list(folds=nnpred,average=avg))

} # end of cv.nn


cv.svm <- function(train){

  library(e1071)
  library(Metrics)

  c <- which(colnames(train)=="target")

    library(caret)
    set.seed(1234)
    flds <- createFolds(train$target,k=5)
    nnpred <- rep(NA,5)

    for(j in 1:5){
      ktest <- train[flds[[j]],]
      ktrain <- train[-flds[[j]],]

      #print("fold")
      #print(j) # tracing progress
      #print(nnpred)

      gam <- 0.01
      cost <- 0.11
      ##The higher the cost produce less support vectors, increases accuracy
      ##However we may overfit
      svmFit = svm (ktrain[,-1], ktrain[,1],
              #type=type, 
              kernel= "radial",
              gamma=gam,
              cost=cost
      )
      
      ##build predictor
      predsvm = predict(svmFit, ktest[,-1])

      mse(ktest[,1],predsvm)
      

      nnpred[j] <- mse(ktest[,1],predsvm)

    } # end of looping cross validations
    avg <- mean(nnpred)

    #print(nnpred)
    #print(avg)
    return(list(folds=nnpred,average=avg))

} # end of cv.svm

```

## NN dividend/price (5 lags for all cases)
```{r, echo=FALSE,message=FALSE}
cv.nn(training)
```

## NN PE10
```{r, echo=FALSE,message=FALSE}
cv.nn(training1)
```

## NN dividend/price + PE10
```{r, echo=FALSE,message=FALSE}
cv.nn(training2)
```

## SVM dividend/price
```{r, echo=FALSE,message=FALSE}
cv.svm(training)
```

## SVM PE10
```{r, echo=FALSE,message=FALSE}
cv.svm(training1)
```

## SVM dividend/price + PE10
```{r, echo=FALSE,message=FALSE}
cv.svm(training2)
```

## Tuning Code

```{r, eval=FALSE}
## h2o deep learning
library(h2o)
localH2O = h2o.init(nthreads=-1)

data_train_h <- as.h2o(data_train,destination_frame = "h2o_data_train")
data_test_h <- as.h2o(data_test,destination_frame = "h2o_data_test")


y <- "target"
x <- setdiff(names(data_train_h), y)


#grid search
hidden_opt <- list(c(200,200), c(100,300,100), c(500,500,500))
l1_opt <- c(1e-5,1e-7)
hyper_params <- list(hidden = hidden_opt, l1 = l1_opt)

model_grid <- h2o.grid("deeplearning",
                        hyper_params = hyper_params,
                        x = (2:ncol(data_train_h)),
                        y = 1,
                        #distribution = "multinomial",
                        training_frame = data_train_h,
                        validation_frame = data_test_h)

# print out the Test MSE for all of the models
for (model_id in model_grid@model_ids) {
  model <- h2o.getModel(model_id)
  mse <- h2o.mse(model, valid = TRUE)
  #mse <- h2o.mse(model, valid = FALSE)
  print(sprintf("Test set MSE: %f", mse))
}

h2o.shutdown()

## SVM

# set up the cross-validated hyper-parameter search
svm_grid_1 = expand.grid(
  cost = 10^c(-1,0.5,1.5,2,3,4),
  gamma = 10^c(-3,-2,-1,0,1,2,3)
)

svm_grid_2 = expand.grid(
  cost = 0.11,
  gamma = 0.01
)

# pack the training control parameters
svm_trcontrol_1 = trainControl(
  method = "cv",
  number = 5,
  verboseIter = TRUE,
  returnData = FALSE,
  returnResamp = "all",               # save losses across all models
  #classProbs = TRUE,                  # set to TRUE for AUC to be computed
  #summaryFunction = twoClassSummary,
  summaryFunction = defaultSummary,
  allowParallel = TRUE
)

svm_train_0 = train(
  x = trainset[,-1],
  y = trainset[,1],
  trControl = svm_trcontrol_1,
  tuneGrid = svm_grid_2,
  method = "svmLinear2"
  #kernel = "radial", #radial is default
  #type="eps-regression"
)

```

# Question 4

# Question 5
## Variance of Portfolio Return 

Key Concepts:

1) $\rho_{ii}\;=\;1 \forall i \in \{ 1, 2 .... n \}$

2) $\rho_{ij}\;= \;\rho_{ji}\;=\;\frac{\mathrm{Cov(X_{i}, X_{j})}}{\sigma_{i} \sigma_{j}} \implies \sigma_{i} \sigma_{j} \rho_{ij}\;=\;Cov(X_{i}, X_{j})$

We begin by looking at the variance when the value of n is 2. 

$Var(a_{1}X_{1} + a_{2}X_{2})\;=\; Var(a_{1}X_{1})\;+\;Var(a_{2}X_{2})\;+\;2Cov(a_{1}X_{1}, a_{2}X_{2})$

$\hspace{1.20in}\;=\; a_{1}^{2}Var(X_{1})\;+\;a_{2}^{2}Var(X_{2})\;+\;2a_{1}a_{2}Cov(X_{1}, X_{2})$

$\hspace{1.20in}\;=\; a_{1}^{2}\sigma_{1}^{2}\;+\;a_{2}^{2}\sigma_{2}^{2}\;+\;2a_{1}a_{2}\sigma_{1} \sigma_{1} \rho_{12}$

$\hspace{1.20in}\;=\;a_{1}^{2}\sigma_{1}^{2}\;+\;a_{1}a_{2}\sigma_{1} \sigma_{1} \rho_{12}\;+\;a_{2}^{2}\sigma_{2}^{2}\;+\;a_{2}a_{1}\sigma_{2} \sigma_{1} \rho_{21}$

$\hspace{1.20in}\;=\;a_{1}a_{1}\sigma_{1}\sigma_{1}\rho_{11}\;+\;a_{1}a_{2}\sigma_{1} \sigma_{1} \rho_{12}\;+\;a_{2}a_{2}\sigma_{2}\sigma_{2}\rho_{22}\;+\;a_{2}a_{1}\sigma_{2} \sigma_{1} \rho_{21}$

$\hspace{1.20in}\;=\;\sum_{j = 1}^{2} a_{1}a_{j}\sigma_{1}\sigma_{j}\rho_{1j}\;+\;\sum_{j = 1}^{2} a_{2}a_{j}\sigma_{2}\sigma_{j}\rho_{2j}$

$\hspace{1.20in}\;=\;\sum_{i = 1}^{2}\sum_{j = 1}^{2} a_{i}a_{j}\sigma_{i}\sigma_{j}\rho_{ij}$

Now, Assume the formula holds for some unspecified value of n = k. It must then be shown that the formula holds for n = k+1, that is:

$$Var\left(\sum_{i = 1}^{k+1} a_{i}X_{i}\right)\;=\; \sum_{i = 1}^{k+1}\sum_{j = 1}^{k+1} a_{i}a_{j}\sigma_{i}\sigma_{j}\rho_{ij}$$

Using the induction hypothesis that the formula holds for n = k, the left-hand side can be rewritten to:

$Var\left(\sum_{i = 1}^{k+1} a_{i}X_{i}\right)\;=\; Var\left(\sum_{i = 1}^{k} a_{i}X_{i} + a_{k+1}X_{k+1}\right)$

$\hspace{1.14in}\;=\; \sum_{i = 1}^{k}\sum_{j = 1}^{k} a_{i}a_{j}\sigma_{i}\sigma_{j}\rho_{ij} + Var(a_{k+1}X_{k+1})\;+\;2Cov(\sum_{i = 1}^{k} a_{i}X_{i},a_{k+1}X_{k+1})$

$\hspace{1.14in}\;=\; \sum_{i = 1}^{k}\sum_{j = 1}^{k} a_{i}a_{j}\sigma_{i}\sigma_{j}\rho_{ij} + a_{k+1}a_{k+1}\sigma_{k+1}\sigma_{k+1}\rho_{k+1k+1}\;+\;2\sum_{i = 1}^{k}Cov( a_{i}X_{i},a_{k+1}X_{k+1})$

$\hspace{1.14in}\;=\; \sum_{i = 1}^{k}\sum_{j = 1}^{k} a_{i}a_{j}\sigma_{i}\sigma_{j}\rho_{ij} + a_{k+1}a_{k+1}\sigma_{k+1}\sigma_{k+1}\rho_{k+1k+1}\;+\;2\sum_{i = 1}^{k}a_{i}a_{k+1}\sigma_{i}\sigma_{k+1}\rho_{ik+1}$

$\hspace{1.14in}\;=\; \sum_{i = 1}^{k+1}\sum_{j = 1}^{k+1} a_{i}a_{j}\sigma_{i}\sigma_{j}\rho_{ij}$

Hence, 

$$Var\left(\sum_{i = 1}^{2} a_{i}X_{i}\right)\;=\; \sum_{i = 1}^{2}\sum_{j = 1}^{2} a_{i}a_{j}\sigma_{i}\sigma_{j}\rho_{ij}$$

$$Var\left(\sum_{i = 1}^{k+1} a_{i}X_{i}\right)\;=\; \sum_{i = 1}^{k+1}\sum_{j = 1}^{k+1} a_{i}a_{j}\sigma_{i}\sigma_{j}\rho_{ij}$$

We can generalize the above result to n terms and conclude: 

$$Var\left(\sum_{i = 1}^{n} a_{i}X_{i}\right)\;=\; \sum_{i = 1}^{n}\sum_{j = 1}^{n} a_{i}a_{j}\sigma_{i}\sigma_{j}\rho_{ij}$$


# Question 6
## Linear Regression on Information Set

Our objective is to prove that the best estimator for $X_{t + h}$ with information set $Z\;=\;\left(X_{t}, X_{t - 1}, ... X_{t - p}\right)$ when all variables follow $X \;\sim\; N(0,1)$ is a linear regression on Z. 

We use $\Sigma$ to denote the variance-covariance matrix of $X_{t + h}$ and $Z$.

$\Sigma\;=\;\mathbb{E}\left[\left(X_{t+h} - \mathbb{E}\left[X_{t + H}\right]\right)^{T}\left(Z - \mathbb{E}\left[Z\right]\right)\right]$

$\hspace{0.15in}\;=\;\mathbb{E}\left[(X_{t + h} - 0)^{T}(Z - 0)\right]$

$\hspace{0.15in}\;=\;\mathbb{E}\left[(X_{t + h}^{T}Z\right]$

Our estimate for $X_{t+h}|Z$ is $\mathbb{E}\left[X_{t + h} | Z\right]$ 

$\mathbb{E}\left[X_{t + h} | Z\right]\;=\;\mathbb{E}X_{t + h}\;+\; \Sigma(\sigma_{Z}^{2})^{-1}\left(Z - \mathbb{E}Z\right)$

$\hspace{0.7in}\;=\;0\;+\;\mathbb{E}\left[X_{t+h}^{T}Z\right]\mathbb{E}\left[Z - \mathbb{E}Z\right]^{-2}\left(Z - 0\right)$

$\hspace{0.7in}\;=\;\mathbb{E}\left[X_{t+h}^{T}Z\right]\mathbb{E}\left[Z^{T}Z\right]^{-1}Z$

The above is a linear regression on $Z$, where $Z$ has the form $Z\;=\;\beta Z\;+\;\epsilon$ where $\beta\;=\;\mathbb{E}\left[X_{t+h}^{T}Z\right]\mathbb{E}\left[Z^{T}Z\right]^{-1}$


